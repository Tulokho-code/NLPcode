{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjv9a2KiYdGk",
        "outputId": "5c5010cc-37cd-4e29-8741-43499f8883ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting spacy==3.8.2\n",
            "  Downloading spacy-3.8.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
            "Collecting torchtext==0.18.0\n",
            "  Downloading torchtext-0.18.0-cp312-cp312-manylinux1_x86_64.whl.metadata (7.9 kB)\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy==3.8.2) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy==3.8.2) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy==3.8.2) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy==3.8.2) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy==3.8.2) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy==3.8.2) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy==3.8.2) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy==3.8.2) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy==3.8.2) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy==3.8.2) (0.4.3)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy==3.8.2) (0.20.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy==3.8.2) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy==3.8.2) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy==3.8.2) (2.12.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy==3.8.2) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy==3.8.2) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy==3.8.2) (25.0)\n",
            "Collecting langcodes<4.0.0,>=3.2.0 (from spacy==3.8.2)\n",
            "  Downloading langcodes-3.5.1-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy==3.8.2) (2.0.2)\n",
            "Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.12/dist-packages (from torchtext==0.18.0) (2.9.0+cu126)\n",
            "Collecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2025.11.3)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.9.0)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (6.0.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.8.2) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.8.2) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.8.2) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.8.2) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.8.2) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.8.2) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.8.2) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.8.2) (2025.11.12)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.0->spacy==3.8.2) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.0->spacy==3.8.2) (0.1.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (3.20.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (3.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (3.5.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy==3.8.2) (8.3.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy==3.8.2) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy==3.8.2) (13.9.4)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy==3.8.2) (0.20.0)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy==3.8.2) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy==3.8.2) (7.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy==3.8.2) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy==3.8.2) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy==3.8.2) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy==3.8.2) (2.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.3.0->torchtext==0.18.0) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy==3.8.2) (0.1.2)\n",
            "Downloading spacy-3.8.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.8/31.8 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchtext-0.18.0-cp312-cp312-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langcodes-3.5.1-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.1/183.1 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: portalocker, langcodes, colorama, sacrebleu, torchtext, spacy\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.8.11\n",
            "    Uninstalling spacy-3.8.11:\n",
            "      Successfully uninstalled spacy-3.8.11\n",
            "Successfully installed colorama-0.4.6 langcodes-3.5.1 portalocker-3.2.0 sacrebleu-2.5.1 spacy-3.8.2 torchtext-0.18.0\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m147.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting fr-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.8.0/fr_core_news_sm-3.8.0-py3-none-any.whl (16.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m130.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fr-core-news-sm\n",
            "Successfully installed fr-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "EN vocab: 9,797 | FR vocab: 10,000\n"
          ]
        }
      ],
      "source": [
        "# %% [code] - Cài đặt và tải dữ liệu\n",
        "!pip install spacy==3.8.2 torchtext==0.18.0 sacrebleu\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download fr_core_news_sm\n",
        "\n",
        "\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "import spacy\n",
        "spacy_en = spacy.load(\"en_core_web_sm\")\n",
        "spacy_fr = spacy.load(\"fr_core_news_sm\")\n",
        "\n",
        "def tokenize_en(text):\n",
        "    return [tok.text.lower() for tok in spacy_en.tokenizer(text)]\n",
        "\n",
        "def tokenize_fr(text):\n",
        "    return [tok.text.lower() for tok in spacy_fr.tokenizer(text)]\n",
        "\n",
        "# Tải dữ liệu Multi30K\n",
        "!wget -q https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/train.en.gz\n",
        "!wget -q https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/train.fr.gz\n",
        "!wget -q https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/val.en.gz\n",
        "!wget -q https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/val.fr.gz\n",
        "!wget -q https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/test_2016_flickr.en.gz\n",
        "!wget -q https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/test_2016_flickr.fr.gz\n",
        "\n",
        "!gunzip -f *.gz\n",
        "\n",
        "def load_lines(file):\n",
        "    with open(file, encoding='utf-8') as f:\n",
        "        return [line.strip() for line in f]\n",
        "\n",
        "train_en = load_lines(\"train.en\")\n",
        "train_fr = load_lines(\"train.fr\")\n",
        "val_en   = load_lines(\"val.en\")\n",
        "val_fr   = load_lines(\"val.fr\")\n",
        "test_en  = load_lines(\"test_2016_flickr.en\")\n",
        "test_fr  = load_lines(\"test_2016_flickr.fr\")\n",
        "\n",
        "# %% [code] - Xây vocab (giới hạn 10k từ phổ biến nhất + 4 special tokens)\n",
        "from collections import Counter\n",
        "\n",
        "def build_vocab(sentences, tokenizer, max_size=10000):\n",
        "    counter = Counter()\n",
        "    for s in sentences:\n",
        "        counter.update(tokenizer(s))\n",
        "\n",
        "    vocab = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2, \"<unk>\": 3}\n",
        "    for word, freq in counter.most_common(max_size - 4):\n",
        "        vocab[word] = len(vocab)\n",
        "    return vocab\n",
        "\n",
        "SRC_vocab = build_vocab(train_en, tokenize_en)\n",
        "TRG_vocab = build_vocab(train_fr, tokenize_fr)\n",
        "\n",
        "print(f\"EN vocab: {len(SRC_vocab):,} | FR vocab: {len(TRG_vocab):,}\")\n",
        "\n",
        "# %% [code] - Dataset + Collate_fn (sort + padding + packing)\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, src_lines, trg_lines):\n",
        "        self.src_lines = src_lines\n",
        "        self.trg_lines = trg_lines\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src_lines)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src = [\"<sos>\"] + tokenize_en(self.src_lines[idx]) + [\"<eos>\"]\n",
        "        trg = [\"<sos>\"] + tokenize_fr(self.trg_lines[idx]) + [\"<eos>\"]\n",
        "\n",
        "        src_ids = [SRC_vocab.get(t, SRC_vocab[\"<unk>\"]) for t in src]\n",
        "        trg_ids = [TRG_vocab.get(t, TRG_vocab[\"<unk>\"]) for t in trg]\n",
        "\n",
        "        return torch.tensor(src_ids), torch.tensor(trg_ids)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    srcs, trgs = zip(*batch)\n",
        "    src_lens = [len(s) for s in srcs]\n",
        "    trg_lens = [len(t) for t in trgs]\n",
        "\n",
        "    srcs_pad = pad_sequence(srcs, batch_first=True, padding_value=SRC_vocab[\"<pad>\"])\n",
        "    trgs_pad = pad_sequence(trgs, batch_first=True, padding_value=TRG_vocab[\"<pad>\"])\n",
        "\n",
        "    return srcs_pad, trgs_pad, src_lens, trg_lens\n",
        "\n",
        "train_dataset = TranslationDataset(train_en, train_fr)\n",
        "val_dataset   = TranslationDataset(val_en, val_fr)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader   = DataLoader(val_dataset,   batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# %% [code] - Model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ccOYXSKzZIlp"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hid_dim, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.lstm = nn.LSTM(emb_dim, hid_dim, batch_first=True)\n",
        "\n",
        "    def forward(self, src, src_len):\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        packed = pack_padded_sequence(embedded, src_len, batch_first=True, enforce_sorted=False)\n",
        "        _, (h, c) = self.lstm(packed)\n",
        "        return h, c\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuXberfzZenQ",
        "outputId": "a1401112-1c6c-4f3f-b062-d4559987e24e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 01 | Train loss: 4.6094 | Val loss: 4.6386\n",
            "Epoch 02 | Train loss: 3.6571 | Val loss: 4.2266\n",
            "Epoch 03 | Train loss: 3.2261 | Val loss: 3.9327\n",
            "Epoch 04 | Train loss: 2.9166 | Val loss: 3.7793\n",
            "Epoch 05 | Train loss: 2.6730 | Val loss: 3.6974\n",
            "Epoch 06 | Train loss: 2.4708 | Val loss: 3.6277\n",
            "Epoch 07 | Train loss: 2.2995 | Val loss: 3.5747\n",
            "Epoch 08 | Train loss: 2.1490 | Val loss: 3.5349\n",
            "Epoch 09 | Train loss: 2.0045 | Val loss: 3.5039\n",
            "Epoch 10 | Train loss: 1.8902 | Val loss: 3.4786\n",
            "Epoch 11 | Train loss: 1.7743 | Val loss: 3.4897\n",
            "Epoch 12 | Train loss: 1.6892 | Val loss: 3.5120\n",
            "Epoch 13 | Train loss: 1.6031 | Val loss: 3.5084\n",
            "Early stopping!\n",
            "un homme joue de la guitare .\n",
            "BLEU score: 57.74\n"
          ]
        }
      ],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hid_dim, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.lstm = nn.LSTM(emb_dim, hid_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hid_dim, vocab_size)\n",
        "\n",
        "    def forward(self, input, h, c, lengths=None):\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        if lengths is not None:\n",
        "            embedded = pack_padded_sequence(embedded, lengths, batch_first=True, enforce_sorted=False)\n",
        "        output, (h, c) = self.lstm(embedded, (h, c))\n",
        "        if lengths is not None:\n",
        "            output, _ = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
        "        logits = self.fc(output)\n",
        "        return logits, h, c\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, src, src_len, trg, trg_len, teacher_forcing_ratio=0.5):\n",
        "        batch_size = src.size(0)\n",
        "        trg_vocab_size = len(TRG_vocab)\n",
        "        max_len = trg.size(1)\n",
        "\n",
        "        outputs = torch.zeros(batch_size, max_len-1, trg_vocab_size).to(src.device)\n",
        "\n",
        "        h, c = self.encoder(src, src_len)\n",
        "\n",
        "        input = trg[:, 0].unsqueeze(1)  # <sos>\n",
        "\n",
        "        for t in range(1, max_len):\n",
        "            logit, h, c = self.decoder(input, h, c)\n",
        "            outputs[:, t-1] = logit.squeeze(1)\n",
        "\n",
        "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
        "            # thay đổi\n",
        "            top1 = logit.argmax(-1).squeeze(1)   # [batch]\n",
        "            input = trg[:, t].unsqueeze(1) if teacher_force else top1.unsqueeze(1)\n",
        "\n",
        "\n",
        "        return outputs\n",
        "\n",
        "# %% [code] - Khởi tạo model\n",
        "enc = Encoder(len(SRC_vocab), emb_dim=256, hid_dim=512, dropout=0.3)\n",
        "dec = Decoder(len(TRG_vocab), emb_dim=256, hid_dim=512, dropout=0.3)\n",
        "model = Seq2Seq(enc, dec).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=TRG_vocab[\"<pad>\"])\n",
        "\n",
        "# %% [code] - Training loop + Early stopping\n",
        "import copy\n",
        "best_val_loss = float('inf')\n",
        "best_model = None\n",
        "patience = 3\n",
        "no_improve = 0\n",
        "\n",
        "for epoch in range(1, 31):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for src, trg, src_len, trg_len in train_loader:\n",
        "        src, trg = src.to(device), trg.to(device)\n",
        "        src_len = torch.tensor(src_len)\n",
        "        trg_len = torch.tensor(trg_len)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, src_len, trg, trg_len, teacher_forcing_ratio=0.5)\n",
        "\n",
        "        loss = criterion(output.reshape(-1, output.size(-1)), trg[:,1:].reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for src, trg, src_len, trg_len in val_loader:\n",
        "            src, trg = src.to(device), trg.to(device)\n",
        "            src_len = torch.tensor(src_len)\n",
        "            output = model(src, src_len, trg, trg_len, teacher_forcing_ratio=0.0)\n",
        "            loss = criterion(output.reshape(-1, output.size(-1)), trg[:,1:].reshape(-1))\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    print(f\"Epoch {epoch:02d} | Train loss: {train_loss/len(train_loader):.4f} | Val loss: {val_loss:.4f}\")\n",
        "\n",
        "    # Early stopping + save best\n",
        "    if val_loss < best_val_loss - 1e-4:\n",
        "        best_val_loss = val_loss\n",
        "        best_model = copy.deepcopy(model.state_dict())\n",
        "        torch.save(best_model, \"best_model.pt\")\n",
        "        no_improve = 0\n",
        "    else:\n",
        "        no_improve += 1\n",
        "        if no_improve >= patience:\n",
        "            print(\"Early stopping!\")\n",
        "            break\n",
        "\n",
        "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
        "\n",
        "# %% [code] - Inference (greedy)\n",
        "def translate(sentence):\n",
        "    model.eval()\n",
        "    tokens = [\"<sos>\"] + tokenize_en(sentence) + [\"<eos>\"]\n",
        "    src = torch.tensor([SRC_vocab.get(t, SRC_vocab[\"<unk>\"]) for t in tokens]).unsqueeze(0).to(device)\n",
        "    src_len = [len(tokens)]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        h, c = model.encoder(src, src_len)\n",
        "\n",
        "    input = torch.tensor([[TRG_vocab[\"<sos>\"]]]).to(device)\n",
        "    result = []\n",
        "    for _ in range(50):\n",
        "        with torch.no_grad():\n",
        "            logit, h, c = model.decoder(input, h, c)\n",
        "            pred = logit.argmax(-1).item()\n",
        "        word = [k for k, v in TRG_vocab.items() if v == pred][0]\n",
        "        if word == \"<eos>\":\n",
        "            break\n",
        "        result.append(word)\n",
        "        input = torch.tensor([[pred]]).to(device)\n",
        "\n",
        "    return \" \".join(result)\n",
        "\n",
        "# Test một câu\n",
        "print(translate(\"A man is playing a guitar.\"))\n",
        "\n",
        "# %% [code] - Tính BLEU score\n",
        "import sacrebleu\n",
        "\n",
        "# model.eval()\n",
        "# refs = [[fr.split()] for fr in test_fr]\n",
        "# preds = []\n",
        "\n",
        "# for en in test_en[:200]:  # test nhanh 200 câu\n",
        "#     pred = translate(en)\n",
        "#     preds.append(pred.split())\n",
        "\n",
        "model.eval()\n",
        "\n",
        "preds = []\n",
        "refs = []\n",
        "\n",
        "for i in range(200):\n",
        "    pred = translate(test_en[i])  # chuỗi\n",
        "    preds.append(pred)\n",
        "\n",
        "    refs.append([test_fr[i]])     # list chứa 1 câu tham chiếu\n",
        "                                  # không split!\n",
        "\n",
        "\n",
        "bleu = sacrebleu.corpus_bleu(preds, refs[:200])\n",
        "print(f\"BLEU score: {bleu.score:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'nn' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 6.2.1 Decoder với Luong Attention\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mAttention\u001b[39;00m(\u001b[43mnn\u001b[49m.Module):\n\u001b[32m      3\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_size):\n\u001b[32m      4\u001b[39m         \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n",
            "\u001b[31mNameError\u001b[39m: name 'nn' is not defined"
          ]
        }
      ],
      "source": [
        "# 6.2.1 Decoder với Luong Attention\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super().__init__()\n",
        "        self.attn = nn.Linear(hidden_size * 2, hidden_size)  # for general (dot) score\n",
        "        self.v = nn.Linear(hidden_size, 1, bias=False)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        # hidden: (1, batch, hidden) - decoder hidden\n",
        "        # encoder_outputs: (src_len, batch, hidden*2) - bi-LSTM\n",
        "        src_len = encoder_outputs.shape[0]\n",
        "        hidden = hidden.repeat(src_len, 1, 1)  # (src_len, batch, hidden)\n",
        "        \n",
        "        # General score: dot product after linear\n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))  # (src_len, batch, hidden)\n",
        "        attention = self.v(energy).squeeze(2)  # (src_len, batch)\n",
        "        return torch.softmax(attention, dim=0)  # (src_len, batch)\n",
        "\n",
        "class DecoderWithAttn(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hidden_size, dropout):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.rnn = nn.LSTM(emb_dim + hidden_size * 2, hidden_size, batch_first=False)  # input cat embed + context (hidden*2 bi)\n",
        "        self.fc_out = nn.Linear(hidden_size * 3, output_dim)  # hidden + embed + context\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.attention = Attention(hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden, cell, encoder_outputs):\n",
        "        # input: (batch)\n",
        "        input = input.unsqueeze(0)  # (1, batch)\n",
        "        embedded = self.dropout(self.embedding(input))  # (1, batch, emb)\n",
        "        \n",
        "        # Attention\n",
        "        attn = self.attention(hidden, encoder_outputs)  # (src_len, batch)\n",
        "        context = torch.bmm(attn.unsqueeze(1), encoder_outputs.permute(1, 0, 2))  # (batch, 1, hidden*2)\n",
        "        context = context.permute(1, 0, 2)  # (1, batch, hidden*2)\n",
        "        \n",
        "        # RNN input: cat embed + context\n",
        "        rnn_input = torch.cat((embedded, context), dim=2)  # (1, batch, emb + hidden*2)\n",
        "        output, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))\n",
        "        \n",
        "        # Output: cat output + embed + context\n",
        "        pred = self.fc_out(torch.cat((output, context, embedded), dim=2).squeeze(0))\n",
        "        return pred, hidden, cell\n",
        "\n",
        "# Seq2Seq với attention (encoder giữ nguyên, decoder mới)\n",
        "class Seq2SeqAttn(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, src_len, trg, trg_len, teacher_forcing_ratio=0.5):\n",
        "        batch_size = src.shape[1]\n",
        "        trg_len_max = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.fc_out.out_features\n",
        "        \n",
        "        outputs = torch.zeros(trg_len_max, batch_size, trg_vocab_size).to(self.device)\n",
        "        encoder_outputs, hidden, cell = self.encoder(src, src_len)  # encoder_outputs for attn\n",
        "        \n",
        "        # Encoder last hidden/cell for decoder init (bi → concat to hidden_size)\n",
        "        hidden = hidden.view(1, batch_size, -1)  # (layers=1, batch, hidden*2 → hidden)\n",
        "        cell = cell.view(1, batch_size, -1)\n",
        "        \n",
        "        input = trg[0, :]  # <sos>\n",
        "        for t in range(1, trg_len_max):\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell, encoder_outputs)\n",
        "            outputs[t] = output\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.argmax(1)\n",
        "            input = trg[t] if teacher_force else top1\n",
        "        return outputs"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
